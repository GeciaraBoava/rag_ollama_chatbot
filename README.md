# RAG Chatbot com Ollama e LlamaIndex

Sistema de chatbot inteligente baseado em RAG (Retrieval-Augmented Generation) que permite consultar documentos locais utilizando modelos de linguagem via Ollama.

## ğŸ“‹ CaracterÃ­sticas

- **Processamento local**: Todos os dados permanecem na sua mÃ¡quina
- **MÃºltiplos formatos**: Suporta PDF, DOCX e TXT
- **Alta performance**: Otimizado para respostas rÃ¡pidas (2-8s)
- **PersistÃªncia**: Ãndice vetorial salvo para reutilizaÃ§Ã£o
- **Interface CLI**: InteraÃ§Ã£o via terminal

## ğŸ”§ Requisitos

### Software necessÃ¡rio

- **Python 3.11.9** (ou superior)
- **Ollama** instalado e configurado
- **Git** (opcional, para clonar o repositÃ³rio)

### InstalaÃ§Ã£o do Ollama

#### Windows/Mac/Linux
```bash
# Acesse: https://ollama.ai/download
# Ou use o instalador oficial para seu sistema operacional
```

ApÃ³s instalar, inicie o servidor:
```bash
ollama serve
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone <url-do-repositorio>
cd rag_ollama_chatbot
```

### 2. Estrutura de pastas
Crie a pasta para seus documentos:
```bash
mkdir documentos
```

A estrutura final deve ser:
```
rag_ollama_chatbot/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ documentos/          # Seus arquivos PDF, DOCX, TXT
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

### 3. Configure o ambiente
Execute o script de setup que criarÃ¡ o ambiente virtual e instalarÃ¡ as dependÃªncias:

```bash
python setup.py
```

O script irÃ¡:
- Criar ambiente virtual (`.venv`)
- Instalar todas as dependÃªncias
- Verificar se o Ollama estÃ¡ ativo
- Baixar o modelo `llama3.2:3b` automaticamente (primeira execuÃ§Ã£o)

## ğŸ“š PreparaÃ§Ã£o dos Documentos

1. Adicione seus documentos na pasta `documentos/`
2. Formatos suportados: `.pdf`, `.docx`, `.txt`
3. NÃ£o hÃ¡ limite de quantidade, mas mais documentos = mais tempo de indexaÃ§Ã£o inicial

Exemplo:
```
documentos/
â”œâ”€â”€ manual_tecnico.pdf
â”œâ”€â”€ relatorio_2024.docx
â”œâ”€â”€ notas.txt
â””â”€â”€ apresentacao.pdf
```

## ğŸ’¬ Uso

### Iniciar o chatbot
```bash
python setup.py
```

### Comandos disponÃ­veis
- Digite sua pergunta e pressione Enter
- `sair`, `exit` ou `quit` para encerrar
- `Ctrl+C` para interromper

### Exemplo de interaÃ§Ã£o
```
ğŸ¤– Chatbot RAG â€” digite sua pergunta
========================================

VocÃª: Qual o conteÃºdo principal do manual tÃ©cnico?
ğŸ” Processando...

ğŸ¤– Bot: O manual tÃ©cnico aborda os seguintes tÃ³picos principais:
1. InstalaÃ§Ã£o do sistema
2. ConfiguraÃ§Ã£o inicial
3. ManutenÃ§Ã£o preventiva
â±ï¸  Tempo: 4.23s

VocÃª: sair
ğŸ‘‹ Encerrando.
```

## ğŸ”„ Reconstruir Ãndice

Se vocÃª adicionar, remover ou modificar documentos, reconstrua o Ã­ndice:

```bash
python setup.py --rebuild
```

Isso irÃ¡:
- Remover Ã­ndice FAISS antigo
- Limpar cache de embeddings
- Reprocessar todos os documentos

## âš™ï¸ ConfiguraÃ§Ãµes AvanÃ§adas

### Alterar modelo LLM

Edite `src/main.py` e modifique:
```python
OLLAMA_MODEL_NAME = "llama3.2:3b"  # Modelo padrÃ£o (rÃ¡pido)
```

Outros modelos disponÃ­veis:
```bash
ollama pull qwen2.5:3b    # Alternativa rÃ¡pida
ollama pull phi3:mini     # Ultra compacto
ollama pull llama3.2:1b   # Mais rÃ¡pido (menos preciso)
```

### Ajustar parÃ¢metros de busca

Em `src/main.py`:
```python
CHUNK_SIZE = 256          # Tamanho dos chunks de texto
SIMILARITY_TOP_K = 2      # Quantidade de chunks recuperados
MAX_TOKENS = 512          # Tamanho mÃ¡ximo da resposta
```

### Usar GPU (se disponÃ­vel)

Modifique o embedding model:
```python
embed_model = HuggingFaceEmbedding(
    model_name=EMBEDDING_MODEL_NAME,
    device="cuda",  # Usar GPU
    cache_folder="./.cache/embeddings"
)
```

## ğŸ“Š Performance

| CenÃ¡rio | Tempo Esperado |
|---------|----------------|
| Primeira indexaÃ§Ã£o | 30s - 2min (depende da quantidade de documentos) |
| Carregamento de Ã­ndice existente | 2-5s |
| Resposta a pergunta | 3-8s |

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Ollama nÃ£o estÃ¡ ativo"
```bash
# Inicie o servidor Ollama
ollama serve
```

### Erro: "Modelo nÃ£o encontrado"
```bash
# Baixe o modelo manualmente
ollama pull llama3.2:3b
```

### Erro: "ImportError: cannot import name..."
```bash
# Reinstale as dependÃªncias
python setup.py
```

### Respostas lentas
- Use um modelo menor: `llama3.2:1b`
- Reduza `SIMILARITY_TOP_K` para 1
- Verifique se hÃ¡ muitos documentos (considere dividir em categorias)

### Pasta `documentos/` vazia
```bash
# O sistema irÃ¡ gerar erro. Adicione pelo menos um documento antes de executar
```

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Documentos    â”‚
â”‚  (PDF/DOCX/TXT) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Processamento  â”‚
â”‚  (Chunking +    â”‚
â”‚   Embeddings)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAISS Vector   â”‚
â”‚     Store       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Engine   â”‚â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚  (Retrieval)    â”‚     â”‚    LLM      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Resposta     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ DependÃªncias Principais

- **llama-index**: Framework RAG
- **ollama**: Cliente Python para Ollama
- **faiss-cpu**: Busca vetorial eficiente
- **sentence-transformers**: GeraÃ§Ã£o de embeddings
- **pypdf/python-docx**: Leitura de documentos

## ğŸ“ LicenÃ§a

Este projeto Ã© fornecido como estÃ¡, sem garantias. Use por sua conta e risco.

## ğŸ¤ ContribuiÃ§Ãµes

SugestÃµes e melhorias sÃ£o bem-vindas. Abra uma issue ou pull request.

## ğŸ“§ Suporte

Para problemas tÃ©cnicos:
1. Verifique a seÃ§Ã£o "SoluÃ§Ã£o de Problemas"
2. Consulte a documentaÃ§Ã£o do Ollama: https://ollama.ai/docs
3. Consulte a documentaÃ§Ã£o do LlamaIndex: https://docs.llamaindex.ai

---

**VersÃ£o**: 1.0.0  
**Ãšltima atualizaÃ§Ã£o**: Outubro 2025